# LLM Provider Configuration
# Choose which provider to use: "ollama", "openai", or "anthropic"
LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=DeepSeek-R1:latest
# When True - use direct API calls, when False - use ollama package
OLLAMA_API_ENABLED=true

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4-turbo-preview

# Anthropic Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-opus-20240229 

# Log Level
LOG_LEVEL=INFO
